---
title: "Data entry, cleaning, and working datasets generation"
author: "MPG"
date: "August 28, 2019"
output: html_document
README: Section order is relevant, prompt errors may be caused by failing to run previous code sections
---


The script will do the following:
1) add  most recent file to aggregated 5min dataset (instructions in 'readme' file)----$'rivername'_'sitename'.csv$
2) download USGS Q data for site/date period----$'rivername'_'sitename'_q$ and $'rivername'_'sitename'_qdaily$
3) generate a 15min averaged dataset with chl, temp, and Q data----$'rivername'_'sitename'_15min.csv$


###########################################################
Modifiy the following information before running the script
```{r}
input_directory = "C:/Users/mpeipoch/Dropbox/COVID_TempDirectories/BrandywineSeston"
output_directory = "C:/Users/mpeipoch/Dropbox/manuscripts/RiverPlankton/working_files"
aggregfile = "BRA_WILMA_Chl_Dec2020.csv"
new_aggregfile = "BRA_WILMA_Chl_Dec2020.csv"
averaged_file = "BRA_WILMA_15min.csv"
q_file = "BRA_WILMA_q.csv"
t_file = "BRA_WILMA_t.csv"
qdaily_file = "BRA_WILMA_qdaily.csv"
tdaily_file = "BRA_WILMA_tdaily.csv"
lastfile = "SL084rawCSV2.1.csv"
startQdate = "2019-03-01"
endQdate = "2020-12-27"
siteQcode = "01481500" #01481000 for BAM and 01481500 for WILMA

```

########################################################
Run the script; knit so partial results can be assessed  
########################################################
```{r,warning=T}
#necessary packages: 
pack_list<-list ("xts","dataRetrieval","dplyr","dataRetrieval","devtools","DataCombine","lubridate","zoo")
for(i in 1:length(pack_list)){
  p <- pack_list[[i]]
  install.packages(p,repos = "https://cran.rstudio.com",dependencies = TRUE)
}
```
```{r}

#####SESTON Data input from .csv and RESEARCH server. File is named by sensor's name only. File include date, time, battery, temperature, chl(ug/L), gain voltage (change chl column from scientific format to general)

library(dplyr)
setwd(output_directory)
chldata = read.csv(aggregfile) ; chldata[,1] <- NULL ; chldata = select (chldata,-c(datetime)) ; tail(chldata,1)
setwd(input_directory)
chldata2 = read.csv(lastfile) ; head(chldata2,1)
chldata = rbind(chldata,chldata2)
chldata$datetime<- as.vector(as.POSIXct(paste(chldata$date, chldata$time), format="%m/%d/%Y %H:%M:%S", tz="EST"))
setwd(output_directory)

#output #1, aggregated dataset of chl and temperature every 5min
write.csv(chldata, file = new_aggregfile) 

#####DISCHARGE data input via URL from USGS using dataRetrieval package


library(dataRetrieval)#check station code online (usually an 8-15 digit numebr)
###checking parameter code
parameterCdFile <- parameterCdFile
Cds = filter(parameterCdFile, grepl("Discharge", parameterCdFile$parameter_nm,ignore.case=TRUE))

###Input request specifications:
siteNo <- siteQcode
pCode <- "00060"
start.date <- startQdate
end.date <- endQdate 
####retrieve data every 15min
        BAM_q <- readNWISuv(siteNumbers = siteNo, 
                            parameterCd = pCode, 
                            startDate = start.date, 
                            endDate = end.date,tz = "EST") 

####retrieve daily average Q
        BAM_q_daily <- readNWISdv(siteNumbers = siteNo, 
                            parameterCd = pCode, 
                            startDate = start.date, 
                            endDate = end.date) 

Qtemp = BAM_q[,3:4]; colnames(Qtemp)<-c("date","Discharge")
write.csv(Qtemp, file = q_file) #output #2, aggregated dataset of Q every 15min
Qtemp = BAM_q_daily[,3:4]; colnames(Qtemp)<-c("date","Discharge")
write.csv(Qtemp, file = qdaily_file) #output #3, aggregated dataset of Q daily

###repeat last steps for Turbidity instead of Q; dates and site remain the same
###checking parameter code
parameterCdFile <- parameterCdFile
Cds = filter(parameterCdFile, grepl("Turbidity", parameterCdFile$parameter_nm,ignore.case=TRUE))

###Input request specifications:
siteNo <- siteQcode
pCode <- "63680"
start.date <- startQdate
end.date <- endQdate 
####retrieve data every 15min
        BAM_t <- readNWISuv(siteNumbers = siteNo, 
                            parameterCd = pCode, 
                            startDate = start.date, 
                            endDate = end.date,tz = "EST") 

####retrieve daily average Q
        BAM_t_daily <- readNWISdv(siteNumbers = siteNo, 
                            parameterCd = pCode, 
                            startDate = start.date, 
                            endDate = end.date) 

Ttemp = BAM_t[,3:4]; colnames(Ttemp) <- c("date","Turbidity")
write.csv(Ttemp, file = t_file) #output #4, aggregated dataset of Turb every 15min
Ttemp = BAM_t_daily[,3:4]; colnames(Ttemp)<-c("date","Turbidity")
write.csv(Ttemp, file = tdaily_file) #output #5, aggregated dataset of Turb daily

#####MOVING AVERAGE SMOOTHING OF ORIGINAL TIME SERIES
#####Generate a merged dataset that contains chl conc. averaged every 15min and paired with discharge data


library(DataCombine)
library(lubridate)
library(zoo)

######################################15min averaged dataset
averaged_df<- select(chldata, c("chl","temp"))
a <- as.vector(chldata$datetime)
averaged_df$date <-as.vector(as.POSIXlt(round(as.double(a)/(15*60))*(15*60),
                                        origin=(as.POSIXlt('1970-01-01')), 
                                        tz="EST"))

#convert date from 'POSIXlt' class to 'double' to allow aggregation with dplyr
averaged_df$date <- as.double(averaged_df$date) 

#aggregate by date 
dataset.15min = averaged_df %>%
  group_by(date) %>%
  summarize(chl.avg = mean(chl, na.rm = TRUE), temp.avg = mean(temp, na.rm = TRUE)) 

#reformat date back to 
dataset.15min$date <- as.POSIXct(dataset.15min$date, format="%m/%d/%Y %H:%M:%S", 
                                           origin=(as.POSIXlt('1970-01-01')),
                                           tz="EST")

#join with USGS data
Q = BAM_q[,3:4]; colnames(Q)<-c("date","Discharge")
Turb = BAM_t[,3:4];  colnames(Turb)<-c("date","Turbidity")
#reformat to datetime format 
Q$date <- as.POSIXct(Q$date, format="%m/%d/%Y %H:%M:%S", tz="EST") 
Turb$date <- as.POSIXct(Turb$date, format="%m/%d/%Y %H:%M:%S", tz="EST") 

QTurb <- inner_join(Q,Turb,by = "date") 
dataset.15min <- left_join(QTurb,dataset.15min,by = "date") #15min averaged dataset

#output #3, aggregated dataset of chl and temperature every 5min
write.csv(dataset.15min, file = averaged_file) 

#Print dates with mismatch between Q and chl data and assess in final dataset
for (i in 1:length(dataset.15min$date)) {
  if(is.element(dataset.15min[i,1],unlist(Q$date))){
    } else {
       print(dataset.15min[i,1]) }
}


```
